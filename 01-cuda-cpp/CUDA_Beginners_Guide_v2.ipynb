{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA C/C++ - A Beginner's Guide\n",
    "\n",
    "Learn GPU programming from the ground up. By the end of this guide, you'll understand how to write parallel code that runs on NVIDIA GPUs.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Why GPUs are fast (and when they're not)\n",
    "2. The CPU-GPU programming model\n",
    "3. Writing and launching GPU kernels\n",
    "4. Thread organization: threads, blocks, and grids\n",
    "5. Memory management between CPU and GPU\n",
    "6. Profiling and debugging CUDA code\n",
    "\n",
    "**Prerequisites:** Basic C programming knowledge. Setup instructions are in [Appendix A](#appendix-a-setup).\n",
    "\n",
    "*Inspired by Mark Harris's [An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why GPU Programming?\n",
    "\n",
    "Consider adding two arrays of 1 billion numbers:\n",
    "\n",
    "```\n",
    "x = [1, 1, 1, ...] (1 billion elements)\n",
    "y = [2, 2, 2, ...] (1 billion elements)\n",
    "result: y = [3, 3, 3, ...]\n",
    "```\n",
    "\n",
    "On a CPU, you'd write a loop that processes elements one by one:\n",
    "\n",
    "```c\n",
    "for (int i = 0; i < 1000000000; i++)\n",
    "    y[i] = x[i] + y[i];\n",
    "```\n",
    "\n",
    "This takes about 15-20 seconds (depends on the CPU obviously). Each addition waits for the previous one to complete.\n",
    "\n",
    "**The insight:** Each addition is independent. Element 0 doesn't need element 1's result. What if we could do all 1 billion additions *at the same time*?\n",
    "\n",
    "That's what GPUs help to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CPU vs GPU: The Mental Model\n",
    "\n",
    "| | CPU | GPU |\n",
    "|---|---|---|\n",
    "| **Design philosophy** | Few fast cores | Many slower cores |\n",
    "| **Core count** | 4-64 cores | 1,000-16,000 cores |\n",
    "| **Optimized for** | Complex sequential tasks | Simple parallel tasks |\n",
    "| **Memory** | System RAM (\"host memory\") | VRAM (\"device memory\") |\n",
    "| **Code terminology** | Host code | Device code / Kernels |\n",
    "\n",
    "**Key insight:** GPUs are fast because they do the *same operation* on *many data points* simultaneously. This is called **data parallelism**.\n",
    "\n",
    "### When GPUs Help (and When They Don't)\n",
    "\n",
    "**Good for GPUs:**\n",
    "- Array/matrix operations (same operation on millions of elements)\n",
    "- Image processing (same filter applied to millions of pixels)\n",
    "- Neural network inference (matrix multiplications)\n",
    "- Physics simulations (same equations for many particles)\n",
    "\n",
    "**Bad for GPUs:**\n",
    "- Sequential algorithms where step N depends on step N-1\n",
    "- Workloads with heavy branching (if/else) that differs per element\n",
    "- Small datasets (overhead exceeds benefit)\n",
    "- Tasks requiring lots of CPU-GPU communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Instance and GPU\n",
    "\n",
    "Let's see what instance type and GPU you're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════════\n",
      "                      INSTANCE DETAILS\n",
      "════════════════════════════════════════════════════════════════\n",
      "Instance Type : g4dn.xlarge\n",
      "Region        : us-east-1\n",
      "\n",
      "Expected GPU (from AWS):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU         : NVIDIA T4\n",
      "  Count       : 1\n",
      "  VRAM        : 16384 MiB\n",
      "\n",
      "════════════════════════════════════════════════════════════════\n",
      "                      DETECTED GPU (by nvidia-smi command)\n",
      "════════════════════════════════════════════════════════════════\n",
      "  GPU         : Tesla T4\n",
      "  VRAM        : 15360 MiB\n",
      "  Compute Cap : 7.5\n",
      "════════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Get instance type via IMDSv2\n",
    "TOKEN=$(curl -s -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\" 2>/dev/null)\n",
    "INSTANCE_TYPE=$(curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/instance-type 2>/dev/null)\n",
    "REGION=$(curl -s -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/placement/region 2>/dev/null)\n",
    "\n",
    "echo \"════════════════════════════════════════════════════════════════\"\n",
    "echo \"                      INSTANCE DETAILS\"\n",
    "echo \"════════════════════════════════════════════════════════════════\"\n",
    "if [ -n \"$INSTANCE_TYPE\" ]; then\n",
    "    echo \"Instance Type : $INSTANCE_TYPE\"\n",
    "    echo \"Region        : $REGION\"\n",
    "    echo \"\"\n",
    "    echo \"Expected GPU (from AWS):\"\n",
    "    aws ec2 describe-instance-types --instance-types $INSTANCE_TYPE --region $REGION \\\n",
    "        --query 'InstanceTypes[0].GpuInfo.Gpus[0].[Name, Manufacturer, Count, MemoryInfo.SizeInMiB]' \\\n",
    "        --output text 2>/dev/null | awk '{printf \"  GPU         : %s %s\\n  Count       : %s\\n  VRAM        : %s MiB\\n\", $2, $1, $3, $4}' \\\n",
    "        || echo \"  No GPU configured for this instance type\"\n",
    "else\n",
    "    echo \"Not running on EC2\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"════════════════════════════════════════════════════════════════\"\n",
    "echo \"                      DETECTED GPU (by nvidia-smi command)\"\n",
    "echo \"════════════════════════════════════════════════════════════════\"\n",
    "if ! command -v nvidia-smi &> /dev/null || ! nvidia-smi &> /dev/null; then\n",
    "    echo \"⚠️  WARNING: No GPU detected!\"\n",
    "    echo \"   This notebook requires an NVIDIA GPU.\"\n",
    "    echo \"   Use a GPU instance (g4dn.xlarge, p3.2xlarge, etc.)\"\n",
    "else\n",
    "    nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv,noheader | \\\n",
    "        awk -F', ' '{printf \"  GPU         : %s\\n  VRAM        : %s\\n  Compute Cap : %s\\n\", $1, $2, $3}'\n",
    "fi\n",
    "echo \"════════════════════════════════════════════════════════════════\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the output:**\n",
    "\n",
    "| Field | Example | Meaning |\n",
    "|-------|---------|--------|\n",
    "| name | Tesla T4 | GPU model |\n",
    "| memory.total | 15360 MiB | VRAM available (~15 GB) |\n",
    "| compute_cap | 7.5 | Architecture version (for compiler flags) |\n",
    "\n",
    "The **compute capability** tells you which features your GPU supports and which compiler flag to use:\n",
    "\n",
    "| Compute Capability | Architecture | Compiler Flag |\n",
    "|-------------------|--------------|---------------|\n",
    "| 7.5 | Turing (T4, RTX 20xx) | `-arch=sm_75` |\n",
    "| 8.0 | Ampere (A100) | `-arch=sm_80` |\n",
    "| 8.6 | Ampere (RTX 30xx) | `-arch=sm_86` |\n",
    "| 8.9 | Ada (RTX 40xx) | `-arch=sm_89` |\n",
    "| 9.0 | Hopper (H100) | `-arch=sm_90` |\n",
    "\n",
    "**Important:** Match your compile flag to your GPU:\n",
    "- Compiling for a **higher** architecture (e.g., `-arch=sm_80` on a sm_75 GPU) will fail with `cudaErrorNoKernelImageForDevice`\n",
    "- Compiling for a **lower** architecture works but may miss optimizations for your GPU\n",
    "- When in doubt, use `-arch=native` (CUDA 11.1+) to auto-detect your GPU\n",
    "\n",
    "Let's see what happens when we compile for the wrong architecture (we will learn error handling in a later section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wrong_arch.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile wrong_arch.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void simpleKernel() {}\n",
    "\n",
    "int main() {\n",
    "    simpleKernel<<<1, 1>>>();\n",
    "    \n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    printf(\"Error code: %d\\n\", err);\n",
    "    printf(\"Error name: %s\\n\", cudaGetErrorName(err));\n",
    "    printf(\"Error desc: %s\\n\", cudaGetErrorString(err));\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 209\n",
      "Error name: cudaErrorNoKernelImageForDevice\n",
      "Error desc: no kernel image is available for execution on the device\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile for sm_90 (Hopper) but run on T4 (sm_75) - this will fail!\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_90 wrong_arch.cu -o wrong_arch && ./wrong_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Your First CUDA Program\n",
    "\n",
    "Let's start with a CPU program that adds two arrays, then convert it to CUDA.\n",
    "\n",
    "### The CPU Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting add_cpu.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_cpu.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "void add(int n, float *x, float *y) {\n",
    "    for (int i = 0; i < n; i++)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 30;  // 1 billion elements (1<<30 = 2^30)\n",
    "    \n",
    "    float *x = malloc(N * sizeof(float));\n",
    "    float *y = malloc(N * sizeof(float));\n",
    "    \n",
    "    // Initialize arrays\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    add(N, x, y);  // Add arrays\n",
    "    \n",
    "    // Verify result (all elements should be 3.0)\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    free(x);\n",
    "    free(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcc add_cpu.c -o add_cpu -lm && ./add_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took 17 seconds in my case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to CUDA: Three Changes\n",
    "\n",
    "To run this on a GPU, we need exactly **three changes**:\n",
    "\n",
    "#### Change 1: Mark the function with `__global__`\n",
    "\n",
    "```c\n",
    "// CPU version\n",
    "void add(int n, float *x, float *y) { ... }\n",
    "\n",
    "// GPU version\n",
    "__global__ void add(int n, float *x, float *y) { ... }\n",
    "```\n",
    "\n",
    "The `__global__` keyword tells the compiler: \"This function runs on the GPU but is called from the CPU.\"\n",
    "\n",
    "Functions marked `__global__` are called **kernels**.\n",
    "\n",
    "#### Change 2: Use CUDA memory allocation\n",
    "\n",
    "```c\n",
    "// CPU version\n",
    "float *x = malloc(N * sizeof(float));\n",
    "free(x);\n",
    "\n",
    "// GPU version (Unified Memory)\n",
    "float *x;\n",
    "cudaMallocManaged(&x, N * sizeof(float));\n",
    "cudaFree(x);\n",
    "```\n",
    "\n",
    "`cudaMallocManaged` allocates **Unified Memory** - memory accessible from both CPU and GPU. The CUDA runtime automatically handles data movement.\n",
    "\n",
    "#### Change 3: Launch with execution configuration\n",
    "\n",
    "```c\n",
    "// CPU version\n",
    "add(N, x, y);\n",
    "\n",
    "// GPU version\n",
    "add<<<1, 1>>>(N, x, y);   // Launch kernel\n",
    "cudaDeviceSynchronize();   // Wait for GPU to finish\n",
    "```\n",
    "\n",
    "The `<<<blocks, threads>>>` syntax specifies how many parallel threads to launch. We'll explore this in detail soon.\n",
    "\n",
    "`cudaDeviceSynchronize()` makes the CPU wait for the GPU to finish - kernel launches are *asynchronous* (the CPU continues immediately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CUDA Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting add_gpu_v1.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_gpu_v1.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    for (int i = 0; i < n; i++)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 30;  // 1 billion elements\n",
    "    float *x, *y;\n",
    "    \n",
    "    // Allocate Unified Memory\n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    // Initialize arrays (on CPU)\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Launch kernel with 1 block, 1 thread\n",
    "    add<<<1, 1>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Verify result\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 add_gpu_v1.cu -o add_gpu_v1 && ./add_gpu_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It works!** But it's actually *slower* than the CPU version (It took my instance 1 minutes and 2 seconds, compared to 17 seconds with CPU). Why? We're only using 1 GPU thread.\n",
    "\n",
    "To make it fast, we need to understand GPU threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. GPU Thread Organization\n",
    "\n",
    "GPUs organize threads into a hierarchy:\n",
    "\n",
    "```\n",
    "Grid (all threads for one kernel launch)\n",
    "└── Block 0\n",
    "│   ├── Thread 0\n",
    "│   ├── Thread 1\n",
    "│   └── ... (up to 1024 threads)\n",
    "├── Block 1\n",
    "│   ├── Thread 0\n",
    "│   ├── Thread 1\n",
    "│   └── ...\n",
    "└── ... (thousands of blocks)\n",
    "```\n",
    "\n",
    "### Why Two Levels?\n",
    "\n",
    "**Threads within a block** can:\n",
    "- Share fast on-chip memory (shared memory)\n",
    "- Synchronize with each other\n",
    "- Cooperate on a task\n",
    "\n",
    "**Threads in different blocks** cannot:\n",
    "- Share memory directly\n",
    "- Synchronize (they may run at different times)\n",
    "\n",
    "This design allows the GPU to schedule blocks independently across its processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Thread Variables\n",
    "\n",
    "Every thread can identify itself using built-in variables:\n",
    "\n",
    "| Variable | Meaning | Example |\n",
    "|----------|---------|--------|\n",
    "| `threadIdx.x` | Thread index within block | \"I'm thread 5 in my block\" |\n",
    "| `blockIdx.x` | Block index within grid | \"I'm in block 2\" |\n",
    "| `blockDim.x` | Threads per block | \"My block has 256 threads\" |\n",
    "| `gridDim.x` | Blocks in grid | \"The grid has 4096 blocks\" |\n",
    "\n",
    "### The Global Index Formula\n",
    "\n",
    "To get a unique index for each thread across the entire grid:\n",
    "\n",
    "```c\n",
    "int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```\n",
    "\n",
    "**Example:** Block 2, Thread 5, with 256 threads per block:\n",
    "```\n",
    "i = 2 * 256 + 5 = 517\n",
    "```\n",
    "\n",
    "This thread processes `array[517]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Thread Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting show_threads.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile show_threads.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void showThreadInfo() {\n",
    "    int globalIdx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    printf(\"Block %d, Thread %d -> Global index: %d\\n\",\n",
    "           blockIdx.x, threadIdx.x, globalIdx);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching 3 blocks x 4 threads = 8 threads:\\n\\n\");\n",
    "    showThreadInfo<<<3, 4>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching 3 blocks x 4 threads = 8 threads:\n",
      "\n",
      "Block 2, Thread 0 -> Global index: 8\n",
      "Block 2, Thread 1 -> Global index: 9\n",
      "Block 2, Thread 2 -> Global index: 10\n",
      "Block 2, Thread 3 -> Global index: 11\n",
      "Block 0, Thread 0 -> Global index: 0\n",
      "Block 0, Thread 1 -> Global index: 1\n",
      "Block 0, Thread 2 -> Global index: 2\n",
      "Block 0, Thread 3 -> Global index: 3\n",
      "Block 1, Thread 0 -> Global index: 4\n",
      "Block 1, Thread 1 -> Global index: 5\n",
      "Block 1, Thread 2 -> Global index: 6\n",
      "Block 1, Thread 3 -> Global index: 7\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 show_threads.cu -o show_threads && ./show_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The output order is unpredictable! Threads run in parallel, not sequentially. Never assume execution order.\n",
    "\n",
    "### Why `.x`?\n",
    "\n",
    "CUDA supports 1D, 2D, and 3D thread layouts. For arrays, 1D (`.x` only) is sufficient. For images, you might use 2D (`.x` and `.y`). For volumes, 3D.\n",
    "\n",
    "```c\n",
    "// 2D example for image processing\n",
    "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Making It Parallel\n",
    "\n",
    "Our first CUDA program used `<<<1, 1>>>` - one thread doing all the work. Let's fix that.\n",
    "\n",
    "### Version 2: One Block, Many Threads\n",
    "\n",
    "With 256 threads, each thread handles every 256th element:\n",
    "\n",
    "```\n",
    "Thread 0: elements 0, 256, 512, 768, ...\n",
    "Thread 1: elements 1, 257, 513, 769, ...\n",
    "Thread 2: elements 2, 258, 514, 770, ...\n",
    "```\n",
    "\n",
    "This is called a **stride loop**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_gpu_v2.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_gpu_v2.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int index = threadIdx.x;          // Starting position (0-255)\n",
    "    int stride = blockDim.x;          // Step size (256)\n",
    "    \n",
    "    for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 30;\n",
    "    float *x, *y;\n",
    "    \n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // 1 block, 256 threads\n",
    "    add<<<1, 256>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 add_gpu_v2.cu -o add_gpu_v2 && ./add_gpu_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! But GPUs have thousands of cores organized into multiple **Streaming Multiprocessors (SMs)**. One block only runs on one SM. We need more blocks.\n",
    "\n",
    "### Version 3: Many Blocks, Many Threads (Full Parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_gpu_v3.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_gpu_v3.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    // Global thread index\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // Total threads in entire grid\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 30;\n",
    "    float *x, *y;\n",
    "    \n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Calculate grid size\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;  // Round up\n",
    "    \n",
    "    printf(\"Launching %d blocks x %d threads = %d total threads\\n\",\n",
    "           numBlocks, blockSize, numBlocks * blockSize);\n",
    "    \n",
    "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching 4194304 blocks x 256 threads = 1073741824 total threads\n",
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 add_gpu_v3.cu -o add_gpu_v3 && ./add_gpu_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Grid-Stride Loop\n",
    "\n",
    "The pattern `for (int i = index; i < n; i += stride)` is called a **grid-stride loop**. It handles arrays of any size:\n",
    "\n",
    "- If `n <= total_threads`: Each thread processes at most 1 element\n",
    "- If `n > total_threads`: Each thread processes multiple elements\n",
    "\n",
    "This is the recommended pattern for CUDA kernels because it's flexible and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Optimal Block Size\n",
    "\n",
    "We used 256 threads per block, but is that optimal? CUDA provides a way to calculate the best block size for your kernel.\n",
    "\n",
    "**Key factors:**\n",
    "- Block size must be a multiple of 32 (warp size)\n",
    "- Maximum is 1024 threads per block\n",
    "- Optimal size depends on kernel's register and shared memory usage\n",
    "\n",
    "Use `cudaOccupancyMaxPotentialBlockSize` to let CUDA calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting optimal_blocksize.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile optimal_blocksize.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 30;\n",
    "    float *x, *y;\n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) { x[i] = 1.0f; y[i] = 2.0f; }\n",
    "    \n",
    "    // Let CUDA calculate optimal block size\n",
    "    int blockSize, minGridSize;\n",
    "    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, add, 0, 0);\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    \n",
    "    printf(\"Optimal block size: %d\\n\", blockSize);\n",
    "    printf(\"Minimum grid size for full occupancy: %d\\n\", minGridSize);\n",
    "    printf(\"Actual grid size: %d\\n\", numBlocks);\n",
    "    \n",
    "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x); cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal block size: 1024\n",
      "Minimum grid size for full occupancy: 40\n",
      "Actual grid size: 1048576\n",
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 optimal_blocksize.cu -o optimal_blocksize && ./optimal_blocksize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple kernels like ours, 256 or 1024 are typically optimal. Complex kernels using more registers or shared memory may need smaller block sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Error Handling\n",
    "\n",
    "CUDA errors are **silent by default**. Your program may appear to work while producing garbage results. This is one of the most common sources of bugs in CUDA programs.\n",
    "\n",
    "### Why CUDA Errors Are Silent\n",
    "\n",
    "CUDA uses asynchronous execution - the CPU doesn't wait for GPU operations to complete. When you call a CUDA function:\n",
    "\n",
    "1. The CPU queues the operation and continues immediately\n",
    "2. The GPU executes it later\n",
    "3. If it fails, the CPU has already moved on\n",
    "\n",
    "This means errors can occur \"in the background\" without crashing your program.\n",
    "\n",
    "### The cudaError_t Type\n",
    "\n",
    "Every CUDA runtime API function returns a `cudaError_t` value. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting check_error_type.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile check_error_type.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    float *d_ptr;\n",
    "    \n",
    "    // cudaMalloc returns cudaError_t - let's capture and inspect it\n",
    "    cudaError_t err = cudaMalloc(&d_ptr, 1024 * sizeof(float));\n",
    "    \n",
    "    printf(\"Return value: %d (cudaSuccess = 0)\\n\", err);\n",
    "    printf(\"Error name: %s\\n\", cudaGetErrorName(err));\n",
    "    printf(\"Error description: %s\\n\", cudaGetErrorString(err));\n",
    "    \n",
    "    // Now let's trigger an error - try to allocate way too much memory\n",
    "    printf(\"\\n--- Triggering an error ---\\n\");\n",
    "    cudaError_t bad_err = cudaMalloc(&d_ptr, (size_t)1024 * 1024 * 1024 * 1024);  // 1 TB!\n",
    "    \n",
    "    printf(\"Return value: %d\\n\", bad_err);\n",
    "    printf(\"Error name: %s\\n\", cudaGetErrorName(bad_err));\n",
    "    printf(\"Error description: %s\\n\", cudaGetErrorString(bad_err));\n",
    "    \n",
    "    cudaFree(d_ptr);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return value: 0 (cudaSuccess = 0)\n",
      "Error name: cudaSuccess\n",
      "Error description: no error\n",
      "\n",
      "--- Triggering an error ---\n",
      "Return value: 2\n",
      "Error name: cudaErrorMemoryAllocation\n",
      "Error description: out of memory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 check_error_type.cu -o check_error_type && ./check_error_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key functions for error handling:\n",
    "- `cudaGetErrorName(err)` - returns the error enum name (e.g., \"cudaErrorMemoryAllocation\")\n",
    "- `cudaGetErrorString(err)` - returns a human-readable description\n",
    "\n",
    "### Two Types of Errors\n",
    "\n",
    "| Error Type | When Detected | How to Check |\n",
    "|------------|---------------|-------------|\n",
    "| **Synchronous** | Immediately (invalid arguments, allocation failures) | Check return value of the API call |\n",
    "| **Asynchronous** | Later (kernel crashes, illegal memory access) | Call `cudaGetLastError()` or `cudaDeviceSynchronize()` |\n",
    "\n",
    "**Kernel launches** (`kernel<<<...>>>()`) don't return `cudaError_t` directly - they queue work and return immediately. Use `cudaGetLastError()` to check if the launch itself failed, and `cudaDeviceSynchronize()` to catch errors during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Checking Macro\n",
    "\n",
    "Writing error checks for every call is tedious. Use a macro:\n",
    "\n",
    "**Note on `__FILE__` and `__LINE__`:** These are special variables built into the C/C++ compiler (called preprocessor macros). Before your code compiles, the compiler automatically replaces `__FILE__` with the current filename as a string, and `__LINE__` with the current line number. This happens at compile time, not runtime - so each error message shows exactly where in your code the problem occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting error_handling.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile error_handling.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "// Error checking macro - wraps CUDA calls and exits on failure\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n",
    "                __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "__global__ void myKernel(float *data) {\n",
    "    data[threadIdx.x] = threadIdx.x;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float *d_data;\n",
    "    \n",
    "    // ========== STEP 1: Allocate GPU memory ==========\n",
    "    // This is SYNCHRONOUS - if it fails, we know immediately\n",
    "    CUDA_CHECK(cudaMalloc(&d_data, 256 * sizeof(float)));\n",
    "    \n",
    "    // ========== STEP 2: Launch kernel ==========\n",
    "    // This is ASYNCHRONOUS - CPU queues work and continues immediately\n",
    "    // The GPU will execute this in the background\n",
    "    myKernel<<<1, 256>>>(d_data);\n",
    "    \n",
    "    // ========== STEP 3: Check for launch errors ==========\n",
    "    // Did the kernel launch fail? (e.g., invalid block size)\n",
    "    // Note: This does NOT wait for the kernel to finish\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    \n",
    "    // ========== STEP 4: Wait and check for execution errors ==========\n",
    "    // Block CPU until GPU finishes, then check for runtime errors\n",
    "    // (e.g., illegal memory access inside the kernel)\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"Kernel executed successfully!\\n\");\n",
    "    \n",
    "    // ========== STEP 5: Cleanup ==========\n",
    "    CUDA_CHECK(cudaFree(d_data));\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel executed successfully!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 error_handling.cu -o error_handling && ./error_handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Check for Errors\n",
    "\n",
    "**Always check:**\n",
    "- `cudaMalloc` / `cudaMallocManaged` - memory allocation can fail\n",
    "- `cudaMemcpy` - data transfer errors\n",
    "- After kernel launches - use `cudaGetLastError()` + `cudaDeviceSynchronize()`\n",
    "\n",
    "**In production code:** Check every CUDA call. The overhead is negligible compared to GPU operations.\n",
    "\n",
    "**During development:** At minimum, add `cudaDeviceSynchronize()` + error check after kernels to catch bugs early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Errors and Their Causes\n",
    "\n",
    "| Error | Typical Cause |\n",
    "|-------|---------------|\n",
    "| `cudaErrorInvalidConfiguration` | Too many threads per block (max 1024) or invalid grid dimensions |\n",
    "| `cudaErrorMemoryAllocation` | Requested more memory than available VRAM |\n",
    "| `cudaErrorIllegalAddress` | Kernel accessed memory outside allocated region |\n",
    "| `cudaErrorInvalidDevice` | Trying to use a GPU that doesn't exist |\n",
    "| `cudaErrorNoKernelImageForDevice` | Compiled for wrong architecture (e.g., sm_80 code on sm_75 GPU) |\n",
    "| `cudaErrorLaunchTimeout` | Kernel took too long (Windows display driver timeout) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Memory Management\n",
    "\n",
    "Memory is typically the bottleneck in GPU programs. Understanding memory types is essential.\n",
    "\n",
    "### Unified Memory vs Explicit Memory\n",
    "\n",
    "So far we've used **Unified Memory** (`cudaMallocManaged`) for simplicity. For production code, **explicit memory management** gives better performance.\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Unified Memory | Simple, automatic | Hidden overhead, less control |\n",
    "| Explicit | Maximum performance | More code, manual management |\n",
    "\n",
    "### Explicit Memory Management (CPU Initialization)\n",
    "\n",
    "This approach initializes data on the CPU, then copies it to the GPU. Use this when data comes from external sources (files, network, user input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting explicit_memory.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile explicit_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 30;  // Using smaller size for this demo\n",
    "    size_t size = N * sizeof(float);\n",
    "    \n",
    "    // Step 1: Allocate host (CPU) memory\n",
    "    float *h_x = (float*)malloc(size);\n",
    "    float *h_y = (float*)malloc(size);\n",
    "    \n",
    "    // Step 2: Initialize on host\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_x[i] = 1.0f;\n",
    "        h_y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Step 3: Allocate device (GPU) memory\n",
    "    float *d_x, *d_y;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMalloc(&d_y, size);\n",
    "    \n",
    "    // Step 4: Copy data from host to device\n",
    "    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_y, h_y, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Step 5: Launch kernel\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    add<<<numBlocks, blockSize>>>(N, d_x, d_y);\n",
    "    \n",
    "    // Step 6: Copy results back to host\n",
    "    cudaMemcpy(h_y, d_y, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(h_y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    // Step 7: Free memory\n",
    "    cudaFree(d_x);\n",
    "    cudaFree(d_y);\n",
    "    free(h_x);\n",
    "    free(h_y);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 explicit_memory.cu -o explicit_memory && ./explicit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU-Side Initialization (Better for Generated Data)\n",
    "\n",
    "When data is generated algorithmically (constants, sequences, random numbers), initialize directly on the GPU. This avoids the CPU→GPU transfer entirely:\n",
    "\n",
    "| Approach | Best For |\n",
    "|----------|----------|\n",
    "| CPU init + copy | Data from files, network, databases, user input |\n",
    "| GPU init | Constants, patterns, random numbers, data already on GPU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gpu_init.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpu_init.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Kernel to initialize arrays directly on GPU\n",
    "__global__ void init(int n, float *x, float *y, float x_val, float y_val) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n) {\n",
    "        x[i] = x_val;\n",
    "        y[i] = y_val;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 30;\n",
    "    size_t size = N * sizeof(float);\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    \n",
    "    // Allocate GPU memory only - no CPU arrays needed!\n",
    "    float *d_x, *d_y;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMalloc(&d_y, size);\n",
    "    \n",
    "    // Initialize directly on GPU - no CPU->GPU transfer!\n",
    "    init<<<numBlocks, blockSize>>>(N, d_x, d_y, 1.0f, 2.0f);\n",
    "    \n",
    "    // Compute\n",
    "    add<<<numBlocks, blockSize>>>(N, d_x, d_y);\n",
    "    \n",
    "    // Only copy back what we need to verify\n",
    "    float *h_y = (float*)malloc(size);\n",
    "    cudaMemcpy(h_y, d_y, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(h_y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(d_x);\n",
    "    cudaFree(d_y);\n",
    "    free(h_y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 gpu_init.cu -o gpu_init && ./gpu_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key difference:** The GPU initialization version skips the `cudaMemcpy` for input data entirely. For large arrays, this can significantly improve performance since CPU↔GPU transfers are often the bottleneck. In my case, it reduced the total time to 11 seconds. \n",
    "\n",
    "### GPU Memory Hierarchy\n",
    "\n",
    "GPUs have several memory types with different speeds and scopes:\n",
    "\n",
    "| Memory | Speed | Scope | Size | Use Case |\n",
    "|--------|-------|-------|------|----------|\n",
    "| Registers | Fastest | Per thread | ~256 KB total | Local variables |\n",
    "| Shared Memory | Very fast | Per block | 48-164 KB | Thread cooperation |\n",
    "| L1/L2 Cache | Fast | Automatic | MB range | Hardware-managed |\n",
    "| Global Memory (VRAM) | Slow | All threads | GB range | Main data storage |\n",
    "\n",
    "For beginners, focus on global memory. Shared memory optimization is an intermediate topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Profiling Your Code\n",
    "\n",
    "**Nsight Systems** (`nsys`) profiles CPU/GPU activity and shows where time is spent.\n",
    "\n",
    "### Basic Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  --------------------------\n",
      "    100.0       2491173667          1  2491173667.0  2491173667.0  2491173667  2491173667          0.0  add(int, float *, float *)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count   Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)               Operation              \n",
      " --------  ---------------  ------  --------  --------  --------  --------  -----------  ------------------------------------\n",
      "     69.6       1541564719  115070   13396.8    3231.0      1503    170813      31894.9  [CUDA memcpy Unified Host-to-Device]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile --stats=true ./add_gpu_v3 2>&1 | grep -A 10 'cuda_gpu_kern_sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "| Column | Meaning |\n",
    "|--------|--------|\n",
    "| Time (%) | Percentage of total GPU time |\n",
    "| Total Time (ns) | Kernel execution time in nanoseconds |\n",
    "| Instances | Number of kernel launches |\n",
    "| Name | Kernel function name |\n",
    "\n",
    "To convert nanoseconds to seconds: divide by 1,000,000,000 (10^9).\n",
    "\n",
    "### Comparing Versions\n",
    "\n",
    "Let's profile all three versions to see the speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Version 1: 1 thread ===\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  -------------  -------------  -----------  -----------  -----------  --------------------------\n",
      "    100.0      45769724681          1  45769724681.0  45769724681.0  45769724681  45769724681          0.0  add(int, float *, float *)\n",
      "\n",
      "\n",
      "=== Version 2: 256 threads (1 block) ===\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  --------------------------\n",
      "    100.0       3464433618          1  3464433618.0  3464433618.0  3464433618  3464433618          0.0  add(int, float *, float *)\n",
      "\n",
      "\n",
      "=== Version 3: Many blocks x 256 threads ===\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  --------------------------\n",
      "    100.0       2493802315          1  2493802315.0  2493802315.0  2493802315  2493802315          0.0  add(int, float *, float *)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"=== Version 1: 1 thread ===\"\n",
    "nsys profile --stats=true ./add_gpu_v1 2>&1 | grep -A 5 'cuda_gpu_kern_sum'\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Version 2: 256 threads (1 block) ===\"\n",
    "nsys profile --stats=true ./add_gpu_v2 2>&1 | grep -A 5 'cuda_gpu_kern_sum'\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Version 3: Many blocks x 256 threads ===\"\n",
    "nsys profile --stats=true ./add_gpu_v3 2>&1 | grep -A 5 'cuda_gpu_kern_sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Common Pitfalls\n",
    "\n",
    "### 1. Forgetting to Synchronize\n",
    "\n",
    "```c\n",
    "// WRONG: Results may not be ready\n",
    "add<<<blocks, threads>>>(N, x, y);\n",
    "printf(\"%f\\n\", y[0]);  // Race condition!\n",
    "\n",
    "// CORRECT\n",
    "add<<<blocks, threads>>>(N, x, y);\n",
    "cudaDeviceSynchronize();  // Wait for GPU\n",
    "printf(\"%f\\n\", y[0]);    // Safe\n",
    "```\n",
    "\n",
    "### 2. Out-of-Bounds Access\n",
    "\n",
    "When total threads exceed array size, add bounds checking:\n",
    "\n",
    "```c\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n)  // Bounds check!\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Integer Overflow in Index Calculation\n",
    "\n",
    "For very large arrays, use `size_t` or `long long`:\n",
    "\n",
    "```c\n",
    "__global__ void process(size_t n, float *data) {\n",
    "    size_t i = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // ...\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. Not Checking Errors\n",
    "\n",
    "Always use error checking (see Section 6). Silent failures are common.\n",
    "\n",
    "### 5. Wrong Architecture Flag\n",
    "\n",
    "```bash\n",
    "# If your GPU is compute capability 7.5 (T4, RTX 2080)\n",
    "nvcc -arch=sm_75 program.cu -o program  # CORRECT\n",
    "nvcc -arch=sm_80 program.cu -o program  # Compiles but may fail at runtime\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary\n",
    "\n",
    "### CPU to CUDA Cheat Sheet\n",
    "\n",
    "| Concept | CPU (C) | GPU (CUDA) |\n",
    "|---------|---------|------------|\n",
    "| Function declaration | `void func()` | `__global__ void func()` |\n",
    "| Memory allocation | `malloc(size)` | `cudaMallocManaged(&ptr, size)` |\n",
    "| Memory free | `free(ptr)` | `cudaFree(ptr)` |\n",
    "| Function call | `func(args)` | `func<<<blocks, threads>>>(args)` |\n",
    "| Wait for completion | (automatic) | `cudaDeviceSynchronize()` |\n",
    "| Thread ID | N/A | `blockIdx.x * blockDim.x + threadIdx.x` |\n",
    "| File extension | `.c` | `.cu` |\n",
    "| Compiler | `gcc` | `nvcc` |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **GPUs excel at data parallelism** - same operation on many elements\n",
    "2. **Threads are organized hierarchically** - threads → blocks → grid\n",
    "3. **Each thread computes its global index** - `blockIdx.x * blockDim.x + threadIdx.x`\n",
    "4. **Use grid-stride loops** for flexible, efficient kernels\n",
    "5. **Always check for errors** - CUDA fails silently\n",
    "6. **Profile before optimizing** - measure, don't guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
    "- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)\n",
    "- [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/index.html)\n",
    "\n",
    "**Free Courses:**\n",
    "- [Fundamentals of Accelerated Computing with CUDA C/C++](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) - NVIDIA DLI\n",
    "- [Fundamentals of Accelerated Computing with CUDA Python](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about) - NVIDIA DLI\n",
    "\n",
    "**Tools:**\n",
    "- `nsys` - Nsight Systems profiler (used in this guide)\n",
    "- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) - Visual profiler\n",
    "- [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) - Kernel profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"appendix-a-setup\"></a>\n",
    "## Appendix A: Setup\n",
    "\n",
    "This appendix covers installing the CUDA development environment. Skip if already set up.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- NVIDIA GPU (any CUDA-capable GPU)\n",
    "- Linux (Ubuntu 22.04/24.04 recommended)\n",
    "- C++ compiler (g++)\n",
    "- Python + Jupyter (for this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Install Python Environment\n",
    "\n",
    "You need Python to run Jupyter notebooks. Several options exist:\n",
    "\n",
    "| Option | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **Miniconda** | Lightweight, conda package manager, easy env management | Separate from system Python | Data science, ML projects |\n",
    "| **Anaconda** | Pre-installed packages, GUI tools | Large download (~3GB) | Beginners who want everything included |\n",
    "| **System Python + pip** | Already installed, simple | Can conflict with system packages | Quick scripts, minimal setup |\n",
    "| **pyenv + pip** | Multiple Python versions, clean isolation | More setup steps | Developers managing multiple projects |\n",
    "\n",
    "We use **Miniconda** here because:\n",
    "- Conda handles complex dependencies (like CUDA libraries) better than pip\n",
    "- Easy to create isolated environments for different projects\n",
    "- Lightweight compared to full Anaconda\n",
    "\n",
    "**Skip this if you already have Python + Jupyter working.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \"$HOME/miniconda3\" ]; then\n",
    "    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "    bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3\n",
    "    $HOME/miniconda3/bin/conda init bash\n",
    "    echo \"Miniconda installed. Run: source ~/.bashrc\"\n",
    "else\n",
    "    echo \"Miniconda already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Install Jupyter Kernel\n",
    "\n",
    "A Jupyter **kernel** is the backend that executes code in notebook cells. Each kernel connects a specific Python environment to Jupyter. Without `ipykernel` installed in your conda environment, Jupyter won't be able to run Python code from that environment.\n",
    "\n",
    "**Skip this if:** You can already run Python cells in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipykernel already installed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! conda list -n base ipykernel 2>/dev/null | grep -q ipykernel; then\n",
    "    conda install -n base ipykernel --update-deps --force-reinstall -y\n",
    "else\n",
    "    echo \"ipykernel already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Install CUDA Toolkit (Ubuntu)\n",
    "\n",
    "The CUDA Toolkit provides:\n",
    "- `nvcc` compiler\n",
    "- CUDA runtime libraries\n",
    "- Header files\n",
    "- Profiling tools (Nsight Systems, Nsight Compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA toolkit already installed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# For Ubuntu 24.04 with CUDA 13.1 (current version)\n",
    "if ! command -v /usr/local/cuda/bin/nvcc &> /dev/null; then\n",
    "    wget -nc https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "    sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "    sudo apt-get update\n",
    "    sudo apt-get -y install cuda-toolkit-13-1\n",
    "else\n",
    "    echo \"CUDA toolkit already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4 Add CUDA to PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA PATH already configured\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! grep -q 'cuda' ~/.bashrc; then\n",
    "    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc\n",
    "    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
    "    echo \"Added CUDA to PATH. Run: source ~/.bashrc\"\n",
    "else\n",
    "    echo \"CUDA PATH already configured\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.5 Install NVIDIA Driver\n",
    "\n",
    "Choose ONE option based on your GPU:\n",
    "\n",
    "**Option 1: Open-source driver** (recommended for datacenter GPUs like T4, V100, A100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA driver already installed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! command -v nvidia-smi &> /dev/null; then\n",
    "    sudo apt-get install -y nvidia-open\n",
    "else\n",
    "    echo \"NVIDIA driver already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Proprietary driver** (for consumer GPUs like RTX series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Uncomment to use proprietary driver instead\n",
    "# sudo apt-get install -y cuda-drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.6 Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking installation:\n",
      "  g++: installed\n",
      "  nvcc: installed\n",
      "  nvidia-smi: installed\n",
      "\n",
      "name, driver_version\n",
      "Tesla T4, 590.48.01\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Checking installation:\"\n",
    "command -v g++ >/dev/null && echo \"  g++: installed\" || echo \"  g++: NOT FOUND\"\n",
    "/usr/local/cuda/bin/nvcc --version >/dev/null 2>&1 && echo \"  nvcc: installed\" || echo \"  nvcc: NOT FOUND\"\n",
    "command -v nvidia-smi >/dev/null && echo \"  nvidia-smi: installed\" || echo \"  nvidia-smi: NOT FOUND\"\n",
    "echo \"\"\n",
    "nvidia-smi --query-gpu=name,driver_version --format=csv 2>/dev/null || echo \"GPU not accessible\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
