{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA C/C++ - A Beginner's Guide\n",
    "\n",
    "Learn GPU programming from the ground up. By the end of this guide, you'll understand how to write parallel code that runs on NVIDIA GPUs.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Why GPUs are fast (and when they're not)\n",
    "2. The CPU-GPU programming model\n",
    "3. Writing and launching GPU kernels\n",
    "4. Thread organization: threads, blocks, and grids\n",
    "5. Memory management between CPU and GPU\n",
    "6. Profiling and debugging CUDA code\n",
    "\n",
    "**Prerequisites:** Basic C programming knowledge. Setup instructions are in [Appendix A](#appendix-a-setup).\n",
    "\n",
    "*Inspired by Mark Harris's [An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why GPU Programming?\n",
    "\n",
    "Consider adding two arrays of 1 billion numbers:\n",
    "\n",
    "```\n",
    "x = [1, 1, 1, ...] (1 billion elements)\n",
    "y = [2, 2, 2, ...] (1 billion elements)\n",
    "result: y = [3, 3, 3, ...]\n",
    "```\n",
    "\n",
    "On a CPU, you'd write a loop that processes elements one by one:\n",
    "\n",
    "```c\n",
    "for (int i = 0; i < 1000000000; i++)\n",
    "    y[i] = x[i] + y[i];\n",
    "```\n",
    "\n",
    "This takes about 15-20 seconds (depends on the CPU obviously). Each addition waits for the previous one to complete.\n",
    "\n",
    "**The insight:** Each addition is independent. Element 0 doesn't need element 1's result. What if we could do all 1 billion additions *at the same time*?\n",
    "\n",
    "That's what GPUs help to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CPU vs GPU: The Mental Model\n",
    "\n",
    "| | CPU | GPU |\n",
    "|---|---|---|\n",
    "| **Design philosophy** | Few fast cores | Many slower cores |\n",
    "| **Core count** | 4-64 cores | 1,000-16,000 cores |\n",
    "| **Optimized for** | Complex sequential tasks | Simple parallel tasks |\n",
    "| **Memory** | System RAM (\"host memory\") | VRAM (\"device memory\") |\n",
    "| **Code terminology** | Host code | Device code / Kernels |\n",
    "\n",
    "**Key insight:** GPUs are fast because they do the *same operation* on *many data points* simultaneously. This is called **data parallelism**.\n",
    "\n",
    "### When GPUs Help (and When They Don't)\n",
    "\n",
    "**Good for GPUs:**\n",
    "- Array/matrix operations (same operation on millions of elements)\n",
    "- Image processing (same filter applied to millions of pixels)\n",
    "- Neural network inference (matrix multiplications)\n",
    "- Physics simulations (same equations for many particles)\n",
    "\n",
    "**Bad for GPUs:**\n",
    "- Sequential algorithms where step N depends on step N-1\n",
    "- Workloads with heavy branching (if/else) that differs per element\n",
    "- Small datasets (overhead exceeds benefit)\n",
    "- Tasks requiring lots of CPU-GPU communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your GPU\n",
    "\n",
    "Let's see what GPU you're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, memory.total [MiB], compute_cap\n",
      "Tesla T4, 15360 MiB, 7.5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the output:**\n",
    "\n",
    "| Field | Example | Meaning |\n",
    "|-------|---------|--------|\n",
    "| name | Tesla T4 | GPU model |\n",
    "| memory.total | 15360 MiB | VRAM available (~15 GB) |\n",
    "| compute_cap | 7.5 | Architecture version (for compiler flags) |\n",
    "\n",
    "The **compute capability** tells you which features your GPU supports and which compiler flag to use:\n",
    "\n",
    "| Compute Capability | Architecture | Compiler Flag |\n",
    "|-------------------|--------------|---------------|\n",
    "| 7.5 | Turing (T4, RTX 20xx) | `-arch=sm_75` |\n",
    "| 8.0 | Ampere (A100) | `-arch=sm_80` |\n",
    "| 8.6 | Ampere (RTX 30xx) | `-arch=sm_86` |\n",
    "| 8.9 | Ada (RTX 40xx) | `-arch=sm_89` |\n",
    "| 9.0 | Hopper (H100) | `-arch=sm_90` |\n",
    "\n",
    "Always match your compile flag to your GPU. Using the wrong one may silently fail at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Your First CUDA Program\n",
    "\n",
    "Let's start with a CPU program that adds two arrays, then convert it to CUDA.\n",
    "\n",
    "### The CPU Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile add_cpu.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "void add(int n, float *x, float *y) {\n",
    "    for (int i = 0; i < n; i++)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 20;  // 1 million elements (1<<20 = 2^20)\n",
    "    \n",
    "    float *x = malloc(N * sizeof(float));\n",
    "    float *y = malloc(N * sizeof(float));\n",
    "    \n",
    "    // Initialize arrays\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    add(N, x, y);  // Add arrays\n",
    "    \n",
    "    // Verify result (all elements should be 3.0)\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    free(x);\n",
    "    free(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcc add_cpu.c -o add_cpu -lm && ./add_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to CUDA: Three Changes\n",
    "\n",
    "To run this on a GPU, we need exactly **three changes**:\n",
    "\n",
    "#### Change 1: Mark the function with `__global__`\n",
    "\n",
    "```c\n",
    "// CPU version\n",
    "void add(int n, float *x, float *y) { ... }\n",
    "\n",
    "// GPU version\n",
    "__global__ void add(int n, float *x, float *y) { ... }\n",
    "```\n",
    "\n",
    "The `__global__` keyword tells the compiler: \"This function runs on the GPU but is called from the CPU.\"\n",
    "\n",
    "Functions marked `__global__` are called **kernels**.\n",
    "\n",
    "#### Change 2: Use CUDA memory allocation\n",
    "\n",
    "```c\n",
    "// CPU version\n",
    "float *x = malloc(N * sizeof(float));\n",
    "free(x);\n",
    "\n",
    "// GPU version (Unified Memory)\n",
    "float *x;\n",
    "cudaMallocManaged(&x, N * sizeof(float));\n",
    "cudaFree(x);\n",
    "```\n",
    "\n",
    "`cudaMallocManaged` allocates **Unified Memory** - memory accessible from both CPU and GPU. The CUDA runtime automatically handles data movement.\n",
    "\n",
    "#### Change 3: Launch with execution configuration\n",
    "\n",
    "```c\n",
    "// CPU version\n",
    "add(N, x, y);\n",
    "\n",
    "// GPU version\n",
    "add<<<1, 1>>>(N, x, y);   // Launch kernel\n",
    "cudaDeviceSynchronize();   // Wait for GPU to finish\n",
    "```\n",
    "\n",
    "The `<<<blocks, threads>>>` syntax specifies how many parallel threads to launch. We'll explore this in detail soon.\n",
    "\n",
    "`cudaDeviceSynchronize()` makes the CPU wait for the GPU to finish - kernel launches are *asynchronous* (the CPU continues immediately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CUDA Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile add_gpu_v1.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    for (int i = 0; i < n; i++)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 20;  // 1 million elements\n",
    "    float *x, *y;\n",
    "    \n",
    "    // Allocate Unified Memory\n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    // Initialize arrays (on CPU)\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Launch kernel with 1 block, 1 thread\n",
    "    add<<<1, 1>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Verify result\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 add_gpu_v1.cu -o add_gpu_v1 && ./add_gpu_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It works!** But it's actually *slower* than the CPU version. Why? We're only using 1 GPU thread - like buying a supercomputer and only using one key on the keyboard.\n",
    "\n",
    "To make it fast, we need to understand GPU threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. GPU Thread Organization\n",
    "\n",
    "GPUs organize threads into a hierarchy:\n",
    "\n",
    "```\n",
    "Grid (all threads for one kernel launch)\n",
    "└── Block 0\n",
    "│   ├── Thread 0\n",
    "│   ├── Thread 1\n",
    "│   └── ... (up to 1024 threads)\n",
    "├── Block 1\n",
    "│   ├── Thread 0\n",
    "│   ├── Thread 1\n",
    "│   └── ...\n",
    "└── ... (thousands of blocks)\n",
    "```\n",
    "\n",
    "### Why Two Levels?\n",
    "\n",
    "**Threads within a block** can:\n",
    "- Share fast on-chip memory (shared memory)\n",
    "- Synchronize with each other\n",
    "- Cooperate on a task\n",
    "\n",
    "**Threads in different blocks** cannot:\n",
    "- Share memory directly\n",
    "- Synchronize (they may run at different times)\n",
    "\n",
    "This design allows the GPU to schedule blocks independently across its processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Thread Variables\n",
    "\n",
    "Every thread can identify itself using built-in variables:\n",
    "\n",
    "| Variable | Meaning | Example |\n",
    "|----------|---------|--------|\n",
    "| `threadIdx.x` | Thread index within block | \"I'm thread 5 in my block\" |\n",
    "| `blockIdx.x` | Block index within grid | \"I'm in block 2\" |\n",
    "| `blockDim.x` | Threads per block | \"My block has 256 threads\" |\n",
    "| `gridDim.x` | Blocks in grid | \"The grid has 4096 blocks\" |\n",
    "\n",
    "### The Global Index Formula\n",
    "\n",
    "To get a unique index for each thread across the entire grid:\n",
    "\n",
    "```c\n",
    "int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```\n",
    "\n",
    "**Example:** Block 2, Thread 5, with 256 threads per block:\n",
    "```\n",
    "i = 2 * 256 + 5 = 517\n",
    "```\n",
    "\n",
    "This thread processes `array[517]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Thread Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile show_threads.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void showThreadInfo() {\n",
    "    int globalIdx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    printf(\"Block %d, Thread %d -> Global index: %d\\n\",\n",
    "           blockIdx.x, threadIdx.x, globalIdx);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching 2 blocks x 4 threads = 8 threads:\\n\\n\");\n",
    "    showThreadInfo<<<2, 4>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 show_threads.cu -o show_threads && ./show_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** The output order is unpredictable! Threads run in parallel, not sequentially. Never assume execution order.\n",
    "\n",
    "### Why `.x`?\n",
    "\n",
    "CUDA supports 1D, 2D, and 3D thread layouts. For arrays, 1D (`.x` only) is sufficient. For images, you might use 2D (`.x` and `.y`). For volumes, 3D.\n",
    "\n",
    "```c\n",
    "// 2D example for image processing\n",
    "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Making It Parallel\n",
    "\n",
    "Our first CUDA program used `<<<1, 1>>>` - one thread doing all the work. Let's fix that.\n",
    "\n",
    "### Version 2: One Block, Many Threads\n",
    "\n",
    "With 256 threads, each thread handles every 256th element:\n",
    "\n",
    "```\n",
    "Thread 0: elements 0, 256, 512, 768, ...\n",
    "Thread 1: elements 1, 257, 513, 769, ...\n",
    "Thread 2: elements 2, 258, 514, 770, ...\n",
    "```\n",
    "\n",
    "This is called a **stride loop**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile add_gpu_v2.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int index = threadIdx.x;          // Starting position (0-255)\n",
    "    int stride = blockDim.x;          // Step size (256)\n",
    "    \n",
    "    for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 20;\n",
    "    float *x, *y;\n",
    "    \n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // 1 block, 256 threads\n",
    "    add<<<1, 256>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 add_gpu_v2.cu -o add_gpu_v2 && ./add_gpu_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! But GPUs have thousands of cores organized into multiple **Streaming Multiprocessors (SMs)**. One block only runs on one SM. We need more blocks.\n",
    "\n",
    "### Version 3: Many Blocks, Many Threads (Full Parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile add_gpu_v3.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    // Global thread index\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // Total threads in entire grid\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 20;\n",
    "    float *x, *y;\n",
    "    \n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Calculate grid size\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;  // Round up\n",
    "    \n",
    "    printf(\"Launching %d blocks x %d threads = %d total threads\\n\",\n",
    "           numBlocks, blockSize, numBlocks * blockSize);\n",
    "    \n",
    "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 add_gpu_v3.cu -o add_gpu_v3 && ./add_gpu_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Grid-Stride Loop\n",
    "\n",
    "The pattern `for (int i = index; i < n; i += stride)` is called a **grid-stride loop**. It handles arrays of any size:\n",
    "\n",
    "- If `n <= total_threads`: Each thread processes at most 1 element\n",
    "- If `n > total_threads`: Each thread processes multiple elements\n",
    "\n",
    "This is the recommended pattern for CUDA kernels because it's flexible and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Optimal Block Size\n",
    "\n",
    "We used 256 threads per block, but is that optimal? CUDA provides a way to calculate the best block size for your kernel.\n",
    "\n",
    "**Key factors:**\n",
    "- Block size must be a multiple of 32 (warp size)\n",
    "- Maximum is 1024 threads per block\n",
    "- Optimal size depends on kernel's register and shared memory usage\n",
    "\n",
    "Use `cudaOccupancyMaxPotentialBlockSize` to let CUDA calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing optimal_blocksize.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile optimal_blocksize.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 20;\n",
    "    float *x, *y;\n",
    "    cudaMallocManaged(&x, N * sizeof(float));\n",
    "    cudaMallocManaged(&y, N * sizeof(float));\n",
    "    \n",
    "    for (int i = 0; i < N; i++) { x[i] = 1.0f; y[i] = 2.0f; }\n",
    "    \n",
    "    // Let CUDA calculate optimal block size\n",
    "    int blockSize, minGridSize;\n",
    "    cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, add, 0, 0);\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    \n",
    "    printf(\"Optimal block size: %d\\n\", blockSize);\n",
    "    printf(\"Minimum grid size for full occupancy: %d\\n\", minGridSize);\n",
    "    printf(\"Actual grid size: %d\\n\", numBlocks);\n",
    "    \n",
    "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    cudaFree(x); cudaFree(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal block size: 1024\n",
      "Minimum grid size for full occupancy: 40\n",
      "Actual grid size: 1024\n",
      "Max error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 optimal_blocksize.cu -o optimal_blocksize && ./optimal_blocksize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple kernels like ours, 256 or 1024 are typically optimal. Complex kernels using more registers or shared memory may need smaller block sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Error Handling\n",
    "\n",
    "CUDA errors are silent by default. Your program may appear to work while producing garbage results. Always check for errors.\n",
    "\n",
    "### Checking CUDA API Calls\n",
    "\n",
    "CUDA functions return `cudaError_t`. Check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile error_handling.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "// Error checking macro\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n",
    "                __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "__global__ void myKernel(float *data) {\n",
    "    data[threadIdx.x] = threadIdx.x;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float *d_data;\n",
    "    \n",
    "    // Good: Check allocation\n",
    "    CUDA_CHECK(cudaMalloc(&d_data, 256 * sizeof(float)));\n",
    "    \n",
    "    // Launch kernel\n",
    "    myKernel<<<1, 256>>>(d_data);\n",
    "    \n",
    "    // Check for kernel launch errors\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    \n",
    "    // Check for kernel execution errors\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"Kernel executed successfully!\\n\");\n",
    "    \n",
    "    CUDA_CHECK(cudaFree(d_data));\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 error_handling.cu -o error_handling && ./error_handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Errors and Their Causes\n",
    "\n",
    "| Error | Typical Cause |\n",
    "|-------|---------------|\n",
    "| `cudaErrorInvalidConfiguration` | Too many threads per block (max 1024) |\n",
    "| `cudaErrorMemoryAllocation` | Requested more memory than available |\n",
    "| `cudaErrorIllegalAddress` | Kernel accessed invalid memory |\n",
    "| `cudaErrorInvalidDevice` | Trying to use a GPU that doesn't exist |\n",
    "| `cudaErrorNoKernelImageForDevice` | Compiled for wrong architecture |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Memory Management\n",
    "\n",
    "Memory is typically the bottleneck in GPU programs. Understanding memory types is essential.\n",
    "\n",
    "### Unified Memory vs Explicit Memory\n",
    "\n",
    "So far we've used **Unified Memory** (`cudaMallocManaged`) for simplicity. For production code, **explicit memory management** gives better performance.\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Unified Memory | Simple, automatic | Hidden overhead, less control |\n",
    "| Explicit | Maximum performance | More code, manual management |\n",
    "\n",
    "### Explicit Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile explicit_memory.cu\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1 << 20;\n",
    "    size_t size = N * sizeof(float);\n",
    "    \n",
    "    // Step 1: Allocate host (CPU) memory\n",
    "    float *h_x = (float*)malloc(size);\n",
    "    float *h_y = (float*)malloc(size);\n",
    "    \n",
    "    // Step 2: Initialize on host\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_x[i] = 1.0f;\n",
    "        h_y[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Step 3: Allocate device (GPU) memory\n",
    "    float *d_x, *d_y;\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMalloc(&d_y, size);\n",
    "    \n",
    "    // Step 4: Copy data from host to device\n",
    "    cudaMemcpy(d_x, h_x, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_y, h_y, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Step 5: Launch kernel\n",
    "    int blockSize = 256;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "    add<<<numBlocks, blockSize>>>(N, d_x, d_y);\n",
    "    \n",
    "    // Step 6: Copy results back to host\n",
    "    cudaMemcpy(h_y, d_y, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(h_y[i] - 3.0f));\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    \n",
    "    // Step 7: Free memory\n",
    "    cudaFree(d_x);\n",
    "    cudaFree(d_y);\n",
    "    free(h_x);\n",
    "    free(h_y);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/usr/local/cuda/bin/nvcc -arch=sm_75 explicit_memory.cu -o explicit_memory && ./explicit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Memory Hierarchy\n",
    "\n",
    "GPUs have several memory types with different speeds and scopes:\n",
    "\n",
    "| Memory | Speed | Scope | Size | Use Case |\n",
    "|--------|-------|-------|------|----------|\n",
    "| Registers | Fastest | Per thread | ~256 KB total | Local variables |\n",
    "| Shared Memory | Very fast | Per block | 48-164 KB | Thread cooperation |\n",
    "| L1/L2 Cache | Fast | Automatic | MB range | Hardware-managed |\n",
    "| Global Memory (VRAM) | Slow | All threads | GB range | Main data storage |\n",
    "\n",
    "For beginners, focus on global memory. Shared memory optimization is an intermediate topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Profiling Your Code\n",
    "\n",
    "**Nsight Systems** (`nsys`) profiles CPU/GPU activity and shows where time is spent.\n",
    "\n",
    "### Basic Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nsys profile --stats=true ./add_gpu_v3 2>&1 | grep -A 10 'cuda_gpu_kern_sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "| Column | Meaning |\n",
    "|--------|--------|\n",
    "| Time (%) | Percentage of total GPU time |\n",
    "| Total Time (ns) | Kernel execution time in nanoseconds |\n",
    "| Instances | Number of kernel launches |\n",
    "| Name | Kernel function name |\n",
    "\n",
    "To convert nanoseconds to seconds: divide by 1,000,000,000 (10^9).\n",
    "\n",
    "### Comparing Versions\n",
    "\n",
    "Let's profile all three versions to see the speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"=== Version 1: 1 thread ===\"\n",
    "nsys profile --stats=true ./add_gpu_v1 2>&1 | grep -A 5 'cuda_gpu_kern_sum'\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Version 2: 256 threads (1 block) ===\"\n",
    "nsys profile --stats=true ./add_gpu_v2 2>&1 | grep -A 5 'cuda_gpu_kern_sum'\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Version 3: Many blocks x 256 threads ===\"\n",
    "nsys profile --stats=true ./add_gpu_v3 2>&1 | grep -A 5 'cuda_gpu_kern_sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Common Pitfalls\n",
    "\n",
    "### 1. Forgetting to Synchronize\n",
    "\n",
    "```c\n",
    "// WRONG: Results may not be ready\n",
    "add<<<blocks, threads>>>(N, x, y);\n",
    "printf(\"%f\\n\", y[0]);  // Race condition!\n",
    "\n",
    "// CORRECT\n",
    "add<<<blocks, threads>>>(N, x, y);\n",
    "cudaDeviceSynchronize();  // Wait for GPU\n",
    "printf(\"%f\\n\", y[0]);    // Safe\n",
    "```\n",
    "\n",
    "### 2. Out-of-Bounds Access\n",
    "\n",
    "When total threads exceed array size, add bounds checking:\n",
    "\n",
    "```c\n",
    "__global__ void add(int n, float *x, float *y) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < n)  // Bounds check!\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Integer Overflow in Index Calculation\n",
    "\n",
    "For very large arrays, use `size_t` or `long long`:\n",
    "\n",
    "```c\n",
    "__global__ void process(size_t n, float *data) {\n",
    "    size_t i = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    // ...\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. Not Checking Errors\n",
    "\n",
    "Always use error checking (see Section 6). Silent failures are common.\n",
    "\n",
    "### 5. Wrong Architecture Flag\n",
    "\n",
    "```bash\n",
    "# If your GPU is compute capability 7.5 (T4, RTX 2080)\n",
    "nvcc -arch=sm_75 program.cu -o program  # CORRECT\n",
    "nvcc -arch=sm_80 program.cu -o program  # Compiles but may fail at runtime\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary\n",
    "\n",
    "### CPU to CUDA Cheat Sheet\n",
    "\n",
    "| Concept | CPU (C) | GPU (CUDA) |\n",
    "|---------|---------|------------|\n",
    "| Function declaration | `void func()` | `__global__ void func()` |\n",
    "| Memory allocation | `malloc(size)` | `cudaMallocManaged(&ptr, size)` |\n",
    "| Memory free | `free(ptr)` | `cudaFree(ptr)` |\n",
    "| Function call | `func(args)` | `func<<<blocks, threads>>>(args)` |\n",
    "| Wait for completion | (automatic) | `cudaDeviceSynchronize()` |\n",
    "| Thread ID | N/A | `blockIdx.x * blockDim.x + threadIdx.x` |\n",
    "| File extension | `.c` | `.cu` |\n",
    "| Compiler | `gcc` | `nvcc` |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **GPUs excel at data parallelism** - same operation on many elements\n",
    "2. **Threads are organized hierarchically** - threads → blocks → grid\n",
    "3. **Each thread computes its global index** - `blockIdx.x * blockDim.x + threadIdx.x`\n",
    "4. **Use grid-stride loops** for flexible, efficient kernels\n",
    "5. **Always check for errors** - CUDA fails silently\n",
    "6. **Profile before optimizing** - measure, don't guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?\n",
    "\n",
    "This guide covered the fundamentals. To continue learning:\n",
    "\n",
    "**Intermediate topics:**\n",
    "- Shared memory for thread cooperation\n",
    "- Memory coalescing for better bandwidth\n",
    "- Streams for overlapping computation and data transfer\n",
    "- Atomic operations\n",
    "\n",
    "**Advanced topics:**\n",
    "- Warp-level programming\n",
    "- Tensor Cores (for deep learning)\n",
    "- Multi-GPU programming\n",
    "- CUDA graphs\n",
    "\n",
    "**Libraries to explore:**\n",
    "- cuBLAS (linear algebra)\n",
    "- cuDNN (deep learning primitives)\n",
    "- Thrust (C++ STL-like parallel algorithms)\n",
    "- CUB (block and warp primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
    "- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)\n",
    "- [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/index.html)\n",
    "\n",
    "**Free Courses:**\n",
    "- [Fundamentals of Accelerated Computing with CUDA C/C++](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) - NVIDIA DLI\n",
    "- [Fundamentals of Accelerated Computing with CUDA Python](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about) - NVIDIA DLI\n",
    "\n",
    "**Tools:**\n",
    "- `nsys` - Nsight Systems profiler (used in this guide)\n",
    "- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) - Visual profiler\n",
    "- [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) - Kernel profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"appendix-a-setup\"></a>\n",
    "## Appendix A: Setup\n",
    "\n",
    "This appendix covers installing the CUDA development environment. Skip if already set up.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- NVIDIA GPU (any CUDA-capable GPU)\n",
    "- Linux (Ubuntu 22.04/24.04 recommended)\n",
    "- C++ compiler (g++)\n",
    "- Python + Jupyter (for this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Install Miniconda\n",
    "\n",
    "Miniconda provides Python and the conda package manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \"$HOME/miniconda3\" ]; then\n",
    "    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "    bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3\n",
    "    $HOME/miniconda3/bin/conda init bash\n",
    "    echo \"Miniconda installed. Run: source ~/.bashrc\"\n",
    "else\n",
    "    echo \"Miniconda already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Install Jupyter Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! conda list -n base ipykernel 2>/dev/null | grep -q ipykernel; then\n",
    "    conda install -n base ipykernel --update-deps --force-reinstall -y\n",
    "else\n",
    "    echo \"ipykernel already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Install CUDA Toolkit (Ubuntu)\n",
    "\n",
    "The CUDA Toolkit provides:\n",
    "- `nvcc` compiler\n",
    "- CUDA runtime libraries\n",
    "- Header files\n",
    "- Profiling tools (Nsight Systems, Nsight Compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# For Ubuntu 24.04 with CUDA 13.1 (current version)\n",
    "if ! command -v /usr/local/cuda/bin/nvcc &> /dev/null; then\n",
    "    wget -nc https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "    sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "    sudo apt-get update\n",
    "    sudo apt-get -y install cuda-toolkit-13-1\n",
    "else\n",
    "    echo \"CUDA toolkit already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4 Add CUDA to PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! grep -q 'cuda' ~/.bashrc; then\n",
    "    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc\n",
    "    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
    "    echo \"Added CUDA to PATH. Run: source ~/.bashrc\"\n",
    "else\n",
    "    echo \"CUDA PATH already configured\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.5 Install NVIDIA Driver\n",
    "\n",
    "Choose ONE option based on your GPU:\n",
    "\n",
    "**Option 1: Open-source driver** (recommended for datacenter GPUs like T4, V100, A100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! command -v nvidia-smi &> /dev/null; then\n",
    "    sudo apt-get install -y nvidia-open\n",
    "else\n",
    "    echo \"NVIDIA driver already installed\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Proprietary driver** (for consumer GPUs like RTX series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Uncomment to use proprietary driver instead\n",
    "# sudo apt-get install -y cuda-drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.6 Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Checking installation:\"\n",
    "command -v g++ >/dev/null && echo \"  g++: installed\" || echo \"  g++: NOT FOUND\"\n",
    "/usr/local/cuda/bin/nvcc --version >/dev/null 2>&1 && echo \"  nvcc: installed\" || echo \"  nvcc: NOT FOUND\"\n",
    "command -v nvidia-smi >/dev/null && echo \"  nvidia-smi: installed\" || echo \"  nvidia-smi: NOT FOUND\"\n",
    "echo \"\"\n",
    "nvidia-smi --query-gpu=name,driver_version --format=csv 2>/dev/null || echo \"GPU not accessible\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Based on Mark Harris's [An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
