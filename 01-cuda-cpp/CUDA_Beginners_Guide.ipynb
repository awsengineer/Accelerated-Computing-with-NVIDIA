{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CUDA C/C++ - A Beginner's Guide\n",
        "\n",
        "This notebook teaches GPU programming with CUDA, explained for programmers familiar with C.\n",
        "\n",
        "Inspired from Mark Harris's [An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/).\n",
        "\n",
        "**What you'll learn:**\n",
        "- How CPU and GPU work together\n",
        "- Writing and running GPU kernels\n",
        "- Parallel programming with threads and blocks\n",
        "- Profiling GPU performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Requirements\n",
        "\n",
        "- CUDA-capable GPU (any NVIDIA GPU)\n",
        "- C++ compiler (g++)\n",
        "- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)\n",
        "- Python environment for Jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Miniconda\n",
        "\n",
        "Miniconda provides Python and the conda package manager. We need it because:\n",
        "- Jupyter notebooks require a Python kernel to execute cells\n",
        "- VS Code's Jupyter extension connects to conda environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "if [ ! -d \"$HOME/miniconda3\" ]; then\n",
        "    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "    bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3\n",
        "    $HOME/miniconda3/bin/conda init bash\n",
        "    echo \"Miniconda installed. Run: source ~/.bashrc\"\n",
        "else\n",
        "    echo \"Miniconda already installed\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install IPython Kernel\n",
        "\n",
        "The IPython kernel (`ipykernel`) bridges Jupyter and Python - required to run notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "if ! conda list -n base ipykernel 2>/dev/null | grep -q ipykernel; then\n",
        "    conda install -n base ipykernel --update-deps --force-reinstall -y\n",
        "else\n",
        "    echo \"ipykernel already installed\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install CUDA Toolkit (Ubuntu 24.04)\n",
        "\n",
        "The CUDA Toolkit provides:\n",
        "- `nvcc` - the NVIDIA CUDA compiler for `.cu` files\n",
        "- CUDA runtime libraries\n",
        "- Header files and APIs\n",
        "- Profiling tools (Nsight Systems, Nsight Compute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "if ! dpkg -l cuda-toolkit-13-1 2>/dev/null | grep -q ^ii; then\n",
        "    wget -nc https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb\n",
        "    sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
        "    sudo apt-get update\n",
        "    sudo apt-get -y install cuda-toolkit-13-1\n",
        "else\n",
        "    echo \"cuda-toolkit-13-1 already installed\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add CUDA to PATH\n",
        "\n",
        "CUDA binaries are installed but not in your shell's PATH:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "if ! grep -q 'cuda' ~/.bashrc; then\n",
        "    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc\n",
        "    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
        "    echo \"Added CUDA to PATH. Run: source ~/.bashrc\"\n",
        "else\n",
        "    echo \"CUDA PATH already configured\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install NVIDIA Driver\n",
        "\n",
        "The driver enables OS-to-GPU communication. Choose ONE option:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Option 1: Open source driver\n",
        "if ! dpkg -l nvidia-open 2>/dev/null | grep -q ^ii; then\n",
        "    sudo apt-get install -y nvidia-open\n",
        "else\n",
        "    echo \"nvidia-open already installed\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tip\n",
        "For your g4dn.xlarge (the instance that I'm using) with NVIDIA T4 GPU, nvidia-open (the open-source kernel modules) is recommeded.\n",
        "\n",
        "Reasons:\n",
        "- NVIDIA officially recommends open kernel modules for datacenter GPUs (Turing and newer, which includes T4)\n",
        "- Better integration with the Linux kernel\n",
        "- Required for some newer features\n",
        "- cuda-drivers installs the proprietary modules which are now considered legacy for datacenter cards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Option 2: Proprietary driver (use this OR the above, not both)\n",
        "if ! dpkg -l cuda-drivers 2>/dev/null | grep -q ^ii; then\n",
        "    # sudo apt-get install -y cuda-drivers\n",
        "    echo \"All good. Open Source drivers are installed instead.\"\n",
        "else\n",
        "    echo \"cuda-drivers already installed. Let's remove them, because open source drives are alerady installed\"\n",
        "    sudo apt-get remove -y cuda-drivers\n",
        "\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ g++ installed\n",
            "✓ nvcc installed\n",
            "✓ nvidia-smi installed\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "command -v g++ >/dev/null && echo \"✓ g++ installed\" || echo \"✗ g++ not found\"\n",
        "/usr/local/cuda/bin/nvcc --version >/dev/null 2>&1 && echo \"✓ nvcc installed\" || echo \"✗ nvcc not found\"\n",
        "command -v nvidia-smi >/dev/null && echo \"✓ nvidia-smi installed\" || echo \"✗ nvidia-smi not found\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Refresher\n",
        "\n",
        "CUDA uses C++, but you only need a few differences from ANSI C:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output: `iostream` vs `stdio.h`\n",
        "\n",
        "| C | C++ |\n",
        "|---|---|\n",
        "| `#include <stdio.h>` | `#include <iostream>` |\n",
        "| `printf(\"x = %d\\n\", x);` | `std::cout << \"x = \" << x << std::endl;` |\n",
        "\n",
        "**Good news:** `printf()` still works in CUDA! Use whichever you prefer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory: `new/delete` vs `malloc/free`\n",
        "\n",
        "| C | C++ |\n",
        "|---|---|\n",
        "| `float *x = malloc(N * sizeof(float));` | `float *x = new float[N];` |\n",
        "| `free(x);` | `delete[] x;` |\n",
        "\n",
        "In CUDA, we'll use `cudaMallocManaged()` instead of both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bit Shift Notation: `1<<20`\n",
        "\n",
        "Same as C! It means 2²⁰ = 1,048,576 (about 1 million).\n",
        "\n",
        "Another example: \n",
        "1<<3 = 1 × 2³\n",
        "2<<3 = 2 × 2³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1<<20 = 1048576\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "echo \"1<<20 = $((1<<20))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: CPU vs GPU - The Mental Model\n",
        "\n",
        "| | CPU | GPU |\n",
        "|---|---|---|\n",
        "| **Cores** | 4-16 fast cores | 1000s of slower cores |\n",
        "| **Good at** | Complex tasks, one at a time | Simple tasks, many at once |\n",
        "| **Memory** | RAM (host memory) | VRAM (device memory) |\n",
        "| **Code name** | Host code | Device code / Kernel |\n",
        "\n",
        "**Key insight:** GPUs are fast because they do the SAME operation on MANY data points simultaneously.\n",
        "\n",
        "**VRAM** (Video RAM) is the dedicated memory on a GPU.\n",
        "\n",
        "It's separate from your system RAM and sits physically on the graphics card, connected directly to the GPU cores via a high-bandwidth bus.\n",
        "\n",
        "Key characteristics:\n",
        "- Much faster than system RAM (hundreds of GB/s vs tens of GB/s)\n",
        "- Limited capacity (8-80 GB on modern GPUs vs 64-512 GB system RAM)\n",
        "- Data must be copied from system RAM to VRAM before the GPU can process it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### My GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name, memory.total [MiB], compute_cap\n",
            "Tesla T4, 15360 MiB, 7.5\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv\n",
        "\n",
        "# SMI = System Management Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example: g4dn.xlarge with Tesla T4**\n",
        "\n",
        "| Field | Value | Meaning |\n",
        "|-------|-------|---------|\n",
        "| name | Tesla T4 | Turing architecture, inference-optimized |\n",
        "| memory.total | 15360 MiB | ~15 GB VRAM |\n",
        "| compute_cap | 7.5 | Compile with `-arch=sm_75` |\n",
        "\n",
        "**Compute Capability** indicates the GPU's architecture generation and supported CUDA features. The T4's 7.5 means Turing architecture. The `sm` in `-arch=sm_75` stands for **Streaming Multiprocessor** - the core processing unit on NVIDIA GPUs. Always match this when compiling:\n",
        "\n",
        "```bash\n",
        "nvcc -arch=sm_75 program.cu -o program\n",
        "```\n",
        "\n",
        "Using the wrong architecture (e.g., `sm_80` for Ampere) will compile successfully but fail silently at runtime.\n",
        "\n",
        "**Why precision matters for T4:**\n",
        "\n",
        "The T4 delivers 8.1 TFLOPS at FP32 but jumps to 65 TFLOPS at FP16 - that's 8x faster! Understanding precision helps you choose the right format:\n",
        "\n",
        "| Format | Name | Bits | Exponent | Mantissa | Range | Precision |\n",
        "|--------|------|------|----------|----------|-------|-----------|\n",
        "| FP32 | Single precision | 32 | 8 | 23 | ±~3.4 × 10³⁸ | ~7-8 digits |\n",
        "| FP16 | Half precision | 16 | 5 | 10 | ±~6.5 × 10⁴ | ~3-4 digits |\n",
        "\n",
        "FP32 is critical for training where gradient accuracy matters. FP16 / INT8 is sufficient for inference since small rounding errors don't affect predictions. The T4 is optimized for the latter, making it ideal for deploying trained models.\n",
        "\n",
        "**Note:** Precision isn't set in the `nvcc` command - `-arch=sm_75` only targets the GPU architecture. Precision is controlled in your code by using `float` (FP32) or `half`/`__half` (FP16) data types, or via libraries like cuBLAS with precision options.\n",
        "\n",
        "**T4 vs Training GPUs:**\n",
        "\n",
        "| GPU | FP32 TFLOPS | Use Case |\n",
        "|-----|-------------|----------|\n",
        "| T4 | 8.1 | Inference, learning CUDA |\n",
        "| V100 | 15.7 | Training |\n",
        "| A100 | 19.5 | Training |\n",
        "| H100 | 67 | Large-scale training |\n",
        "| H200 | 67 | Large models (141 GB HBM3e) |\n",
        "\n",
        "The T4 excels at FP16/INT8 inference but has lower FP32 throughput than training GPUs. This makes it ideal for deploying trained models in production or learning CUDA at significantly lower cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your GPU specs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Starting Simple - Array Addition\n",
        "\n",
        "We'll add two arrays element by element:\n",
        "```\n",
        "x = [1, 1, 1, ...] (1 billion elements)\n",
        "y = [2, 2, 2, ...] (1 billion elements)\n",
        "result: y = [3, 3, 3, ...]\n",
        "```\n",
        "\n",
        "First, let's do it on the CPU (pure C - this should look familiar):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile add.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// Function to add elements of two arrays\n",
        "void add(int n, float *x, float *y) {\n",
        "    for (int i = 0; i < n; i++)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1<<30;  // 1 billion elements\n",
        "    \n",
        "    float *x = malloc(N * sizeof(float));\n",
        "    float *y = malloc(N * sizeof(float));\n",
        "    \n",
        "    // Initialize: x=1, y=2\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        x[i] = 1.0f;\n",
        "        y[i] = 2.0f;\n",
        "    }\n",
        "    \n",
        "    add(N, x, y);  // Do the addition\n",
        "    \n",
        "    // Check for errors (all values should be 3.0)\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < N; i++)\n",
        "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "    \n",
        "    free(x);\n",
        "    free(y);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max error: 0.000000\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "gcc add.c -o add -lm && ./add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The opportunity:** This loop runs 1 billion iterations sequentially - it works, but each addition waits for the previous one (~17 seconds on this machine's CPU). What if we could do all additions AT THE SAME TIME?\n",
        "\n",
        "That's where the GPU comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Your First CUDA Program\n",
        "\n",
        "To run code on the GPU, we need **3 changes**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change 1: Mark the function with `__global__`\n",
        "\n",
        "**CPU version:**\n",
        "```c\n",
        "void add(int n, float *x, float *y) { ... }\n",
        "```\n",
        "\n",
        "**GPU version - add `__global__`:**\n",
        "```c\n",
        "__global__ void add(int n, float *x, float *y) { ... }\n",
        "```\n",
        "\n",
        "`__global__` tells the compiler: \"This function runs on the GPU but is called from the CPU.\"\n",
        "\n",
        "These functions are called **kernels**. Code on GPU = **device code**, code on CPU = **host code**.\n",
        "\n",
        "**Important:** `__global__` is a **function qualifier** - the entire function body (everything between `{` and `}`) runs on GPU:\n",
        "\n",
        "```c\n",
        "__global__\n",
        "void add(int n, float *x, float *y) {   // Scope starts\n",
        "    for (int i = 0; i < n; i++)         // GPU code\n",
        "        y[i] = x[i] + y[i];             // GPU code\n",
        "}                                        // Scope ends\n",
        "```\n",
        "\n",
        "You cannot mark a block of code inside a function to run on GPU - you must create a separate `__global__` function and call it with `<<<blocks, threads>>>`.\n",
        "\n",
        "**CUDA function qualifiers:**\n",
        "\n",
        "| Qualifier | Runs on | Called from | Use case |\n",
        "|-----------|---------|-------------|----------|\n",
        "| `__global__` | GPU | CPU (or GPU with dynamic parallelism) | Kernel entry points |\n",
        "| `__device__` | GPU | GPU only | Helper functions called by kernels |\n",
        "| `__host__` | CPU | CPU | Regular CPU functions (default, optional) |\n",
        "\n",
        "**Combining qualifiers:**\n",
        "```c\n",
        "__host__ __device__ float square(float x) { return x * x; }\n",
        "```\n",
        "This compiles the function for both CPU and GPU - useful for utility functions you need in both places."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change 2: Use Unified Memory\n",
        "\n",
        "```c\n",
        "// CPU only\n",
        "float *x = malloc(N * sizeof(float));\n",
        "free(x);\n",
        "\n",
        "// CPU + GPU (Unified Memory)\n",
        "float *x;\n",
        "cudaMallocManaged(&x, N * sizeof(float));\n",
        "cudaFree(x);\n",
        "```\n",
        "\n",
        "[Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) creates memory accessible by both CPU and GPU automatically.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "Despite the name, Unified Memory isn't truly \"shared\" - data physically copies between CPU RAM and GPU VRAM. What's unified is the **address space** (single pointer works everywhere), not the memory itself:\n",
        "\n",
        "1. CPU writes to array → data lives in CPU RAM\n",
        "2. GPU kernel launches → data copies to GPU VRAM over PCIe\n",
        "3. CPU reads results → data copies back to CPU RAM\n",
        "\n",
        "**What is an address space?**\n",
        "\n",
        "An address space is the range of memory addresses a processor can use. Think of it like street addresses - CPU has addresses 0x0000-0xFFFF for its RAM, GPU has its own addresses for VRAM. Normally these are separate: a pointer valid on CPU means nothing to the GPU.\n",
        "\n",
        "Unified Memory creates a **single virtual address space** where one pointer (e.g., `0x7f3a...`) is valid on both. The CUDA runtime translates this to the actual physical location and copies data as needed.\n",
        "\n",
        "**Note:** CPU and GPU don't need the same amount of memory. The address space is virtual - you can allocate more than your GPU's VRAM (15 GB on T4). Overflow stays in CPU RAM and pages into VRAM as needed. On multi-GPU systems, overflow still goes to CPU RAM (not other GPUs) - the benefit is simpler programming, not combined VRAM capacity.\n",
        "\n",
        "**Why true sharing is impossible:**\n",
        "\n",
        "CPU RAM sits on the motherboard, connected to the CPU. GPU VRAM sits on the graphics card, connected to the GPU. They're physically separate chips connected only by PCIe (a relatively slow bus). For true sharing, both processors would need to access the same memory chips - which would require them to be on the same silicon or share a memory bus. Some APUs (Accelerated Processing Units - chips combining CPU and GPU on the same die, like PlayStation 5 or AMD Ryzen with integrated graphics) do this, but discrete GPUs like your T4 cannot.\n",
        "\n",
        "This automatic migration has overhead. For production code, explicit memory management gives more control and better performance:\n",
        "\n",
        "**Step 1: Allocate memory on both CPU and GPU**\n",
        "```c\n",
        "float *h_x = malloc(N * sizeof(float));  // Host (CPU) memory\n",
        "float *d_x;                               // Device (GPU) pointer\n",
        "cudaMalloc(&d_x, N * sizeof(float));     // Allocate on GPU\n",
        "```\n",
        "\n",
        "**Step 2: Initialize data on CPU**\n",
        "```c\n",
        "for (int i = 0; i < N; i++) h_x[i] = 1.0f;\n",
        "```\n",
        "\n",
        "**Step 3: Copy data from CPU to GPU**\n",
        "```c\n",
        "cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "```\n",
        "\n",
        "**Step 4: Run kernel on GPU**\n",
        "```c\n",
        "kernel<<<blocks, threads>>>(d_x);\n",
        "```\n",
        "\n",
        "**Step 5: Copy results back from GPU to CPU**\n",
        "```c\n",
        "cudaMemcpy(h_x, d_x, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "```\n",
        "\n",
        "**Step 6: Free memory**\n",
        "```c\n",
        "cudaFree(d_x);  // Free GPU memory\n",
        "free(h_x);      // Free CPU memory\n",
        "```\n",
        "\n",
        "This is faster because you control exactly when copies happen, and can overlap computation with data transfer.\n",
        "\n",
        "**When to use which:**\n",
        "- **Unified Memory:** Simpler code, good for learning, prototyping, unpredictable access patterns, datasets larger than VRAM, or multi-GPU systems (runtime automatically migrates data between GPUs - no manual `cudaMemcpyPeer` or tracking which data is where)\n",
        "- **Explicit (`cudaMalloc` + `cudaMemcpy`):** Maximum performance when you know exactly when data needs to move\n",
        "\n",
        "**Caution on multi-GPU:** If multiple GPUs frequently access the same data, it keeps copying back and forth (\"thrashing\"), hurting performance. With NVLink (high-end systems), copies go directly between GPUs. Without NVLink (most systems, including g4dn), copies route through CPU RAM (GPU0 → RAM → GPU1), which is slower. Explicit management lets you control which data stays on which GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change 3: Launch with `<<<blocks, threads>>>`\n",
        "\n",
        "```c\n",
        "// CPU call\n",
        "add(N, x, y);\n",
        "\n",
        "// GPU call\n",
        "add<<<1, 1>>>(N, x, y);      // Launch kernel\n",
        "cudaDeviceSynchronize();      // Wait for GPU to finish\n",
        "```\n",
        "\n",
        "**Anatomy of a kernel launch:**\n",
        "\n",
        "```\n",
        "add<<<1, 1>>>(N, x, y);\n",
        "│   │  │   │  └─────── Function arguments (same as CPU)\n",
        "│   │  │   └────────── Threads per block\n",
        "│   │  └────────────── Number of blocks\n",
        "│   └───────────────── Execution configuration (CUDA-specific)\n",
        "└───────────────────── Kernel function name\n",
        "```\n",
        "\n",
        "The `<<<blocks, threads>>>` syntax is CUDA's way of specifying parallelism:\n",
        "- `<<<1, 1>>>` = 1 block × 1 thread = 1 total thread (sequential, slow)\n",
        "- `<<<1, 256>>>` = 1 block × 256 threads = 256 parallel threads\n",
        "- `<<<4096, 256>>>` = 4096 blocks × 256 threads = ~1 million parallel threads\n",
        "\n",
        "`cudaDeviceSynchronize()` blocks the CPU until the GPU finishes - necessary because kernel launches are asynchronous (CPU continues immediately without waiting).\n",
        "\n",
        "**Why synchronization matters:**\n",
        "\n",
        "```c\n",
        "add<<<blocks, threads>>>(N, x, y);  // CPU sends work to GPU and continues immediately\n",
        "printf(\"%f\", y[0]);                 // BUG: GPU might not be done yet!\n",
        "\n",
        "// Correct:\n",
        "add<<<blocks, threads>>>(N, x, y);  // CPU sends work to GPU\n",
        "cudaDeviceSynchronize();             // CPU waits here until GPU finishes\n",
        "printf(\"%f\", y[0]);                 // Safe: GPU is definitely done\n",
        "```\n",
        "\n",
        "Without synchronization, the CPU might read results before the GPU has written them - leading to incorrect or garbage values.\n",
        "\n",
        "**Note:** `cudaDeviceSynchronize()` blocks the CPU thread - those cycles are wasted waiting. If you have CPU work that doesn't depend on GPU results, do it before syncing:\n",
        "\n",
        "```c\n",
        "kernel<<<blocks, threads>>>(d_x);  // GPU starts, CPU continues immediately\n",
        "prepare_next_batch();              // CPU work that doesn't need GPU results\n",
        "write_logs();                      // More independent work\n",
        "cudaDeviceSynchronize();           // NOW wait for GPU\n",
        "use_results(d_x);                  // Safe to use GPU results\n",
        "```\n",
        "\n",
        "If your CPU work depends on GPU output, you must wait - this optimization only helps when you have independent work.\n",
        "\n",
        "Advanced: CUDA streams and events allow even finer control over async operations - but that's beyond this beginner's guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CUDA code in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile add.cu\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function - runs on GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y) {\n",
        "    for (int i = 0; i < n; i++)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1<<30;  // 1 billion elements\n",
        "    float *x, *y;\n",
        "    \n",
        "    // Allocate Unified Memory - accessible from CPU or GPU\n",
        "    cudaMallocManaged(&x, N * sizeof(float));\n",
        "    cudaMallocManaged(&y, N * sizeof(float));\n",
        "    \n",
        "    // Initialize on CPU (simple, but slow for large arrays. (We will improve it later)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        x[i] = 1.0f;\n",
        "        y[i] = 2.0f;\n",
        "    }\n",
        "    \n",
        "    // Run kernel on GPU (1 block, 1 thread - intentionally slow!)\n",
        "    add<<<1, 1>>>(N, x, y);\n",
        "    \n",
        "    // Wait for GPU to finish before accessing results\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    // Check for errors (using CPU, for now)\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < N; i++)\n",
        "       maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "    \n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile with `nvcc` (NVIDIA CUDA Compiler) and run:\n",
        "\n",
        "**What is nvcc?** NVIDIA's compiler for CUDA code. It separates CPU code from GPU code, compiles GPU code to machine instructions for your GPU, passes CPU code to `g++`, and links everything into one executable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max error: 0.000000\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "/usr/local/cuda/bin/nvcc add.cu -o add_cuda && ./add_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**It works!** But this is actually much SLOWER than CPU, because we're only using 1 GPU thread.\n",
        "\n",
        "You might think: \"Just change `<<<1, 1>>>` to `<<<1, 256>>>` for more threads!\" But that won't help with this kernel. Look at its loop:\n",
        "\n",
        "```c\n",
        "for (int i = 0; i < n; i++)  // Every thread would run this same loop!\n",
        "```\n",
        "\n",
        "With `<<<1, 256>>>`, all 256 threads would execute the **same loop** from 0 to N:\n",
        "- Thread 0: processes elements 0, 1, 2, ... N-1\n",
        "- Thread 1: processes elements 0, 1, 2, ... N-1\n",
        "- Thread 2: processes elements 0, 1, 2, ... N-1\n",
        "- (all 256 threads do identical work)\n",
        "\n",
        "That's 256x the work for no benefit! We need to rewrite the kernel so each thread handles **different** elements using `threadIdx.x`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Profiling with Nsight Systems\n",
        "\n",
        "**Nsight Systems** is NVIDIA's system-wide profiler that shows CPU/GPU activity, memory transfers, and kernel timings on a unified timeline. The `nsys` command is its CLI tool.\n",
        "\n",
        "Let's measure how long the kernel takes:\n",
        "\n",
        "```bash\n",
        "nsys profile --stats=true ./add_cuda\n",
        "```\n",
        "\n",
        "This generates two files:\n",
        "- **`.nsys-rep`** - Native report format for the Nsight Systems GUI (`nsys-ui`)\n",
        "- **`.sqlite`** - SQLite database for programmatic queries\n",
        "\n",
        "The `.sqlite` file is created automatically when you use `--stats=true` because nsys computes statistics via SQL queries internally. You can also explicitly request it with `--export=sqlite`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)             Name           \n",
            " --------  ---------------  ---------  -------------  -------------  -----------  -----------  -----------  --------------------------\n",
            "    100.0      45828070907          1  45828070907.0  45828070907.0  45828070907  45828070907          0.0  add(int, float *, float *)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)               Operation              \n",
            " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------------------------\n",
            "     68.3       1449746401  49152   29495.2   10223.0      1855    169789      48194.8  [CUDA memcpy Unified Host-to-Device]\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "nsys profile --stats=true ./add_cuda 2>&1 | grep -A 10 'cuda_gpu_kern_sum'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reading nsys output:**\n",
        "\n",
        "The `cuda_gpu_kern_sum` section shows **only GPU kernel execution time**, not total program time:\n",
        "\n",
        "| Column | Meaning |\n",
        "|--------|--------|\n",
        "| Time (%) | Percentage of total GPU time spent in this kernel |\n",
        "| Total Time (ns) | Kernel execution time in nanoseconds (divide by 1,000,000,000 for seconds) |\n",
        "| Instances | How many times the kernel was called |\n",
        "| Name | Kernel function name with parameters |\n",
        "\n",
        "**Why total program time is longer than kernel time:**\n",
        "\n",
        "The program does more than just run the kernel:\n",
        "1. CPU initialization loop (1 billion iterations) - ~15-20 seconds\n",
        "2. GPU kernel execution - shown in nsys output\n",
        "3. CPU error-checking loop - a few seconds\n",
        "\n",
        "**Breaking down the numbers:**\n",
        "\n",
        "The nsys output shows `Total Time (ns)` for the kernel. To convert nanoseconds to seconds, divide by 1,000,000,000 (or 10⁹). For example, `45814840481 ns ÷ 10⁹ = ~45.8 seconds`.\n",
        "\n",
        "If you time the entire program (e.g., with `time ./add_cuda`), you'll see it takes longer than the kernel time alone. The difference is CPU work: initialization loops, error checking, and CUDA setup overhead.\n",
        "\n",
        "With 1 billion elements and `<<<1, 1>>>`, the `add` kernel will be very slow. Let's make it faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Understanding GPU Threads\n",
        "\n",
        "### Thread Organization\n",
        "\n",
        "Threads are grouped into **blocks**, and blocks form a **grid**:\n",
        "\n",
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\" width=\"700\">\n",
        "\n",
        "Why two levels? Threads in the same block can share fast memory and synchronize. Threads across blocks cannot.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Threads Find Their Work\n",
        "\n",
        "Each thread has built-in variables to identify itself:\n",
        "\n",
        "| Variable | What it tells you |\n",
        "|----------|-------------------|\n",
        "| `threadIdx.x` | \\\"I'm thread #5 in my block\\\" |\n",
        "| `blockIdx.x` | \\\"I'm in block #2\\\" |\n",
        "| `blockDim.x` | \\\"My block has 256 threads\\\" |\n",
        "\n",
        "**The Key Formula:**\n",
        "```c\n",
        "int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "```\n",
        "\n",
        "**Example:** If you're thread 5 in block 2, and blocks have 256 threads:\n",
        "```\n",
        "i = 2 * 256 + 5 = 517\n",
        "```\n",
        "So you process `array[517]`.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why `.x`?\n",
        "\n",
        "CUDA supports 1D, 2D, and 3D layouts. The `.x` selects the dimension:\n",
        "\n",
        "| Layout | Use case | Variables |\n",
        "|--------|----------|----------|\n",
        "| 1D | Arrays | `.x` |\n",
        "| 2D | Images | `.x`, `.y` |\n",
        "| 3D | Volumes | `.x`, `.y`, `.z` |\n",
        "\n",
        "For arrays, 1D is enough - we only use `.x`.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's see thread IDs in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting show_threads.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile show_threads.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void showThreads() {\n",
        "    int globalId = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    printf(\"Block %d, Thread %d -> Global ID: %d\\n\",\n",
        "           blockIdx.x, threadIdx.x, globalId);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"Launching 2 blocks x 4 threads = 8 threads total:\\n\\n\");\n",
        "    showThreads<<<2, 4>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching 2 blocks x 4 threads = 8 threads total:\n",
            "\n",
            "Block 1, Thread 0 -> Global ID: 4\n",
            "Block 1, Thread 1 -> Global ID: 5\n",
            "Block 1, Thread 2 -> Global ID: 6\n",
            "Block 1, Thread 3 -> Global ID: 7\n",
            "Block 0, Thread 0 -> Global ID: 0\n",
            "Block 0, Thread 1 -> Global ID: 1\n",
            "Block 0, Thread 2 -> Global ID: 2\n",
            "Block 0, Thread 3 -> Global ID: 3\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "/usr/local/cuda/bin/nvcc show_threads.cu -o show_threads && ./show_threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notice:** Output order is random! Threads run in parallel, not sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Parallelizing with One Block\n",
        "\n",
        "Let's use 256 threads in one block. Each thread handles a portion of the array:\n",
        "\n",
        "```\n",
        "Thread 0: processes elements 0, 256, 512, ...\n",
        "Thread 1: processes elements 1, 257, 513, ...\n",
        "Thread 2: processes elements 2, 258, 514, ...\n",
        "```\n",
        "\n",
        "This is called a **stride loop** - each thread strides through the array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The kernel with stride:\n",
        "\n",
        "```c\n",
        "__global__\n",
        "void add(int n, float *x, float *y) {\n",
        "    int index = threadIdx.x;      // Starting position\n",
        "    int stride = blockDim.x;      // Step size (256)\n",
        "    for (int i = index; i < n; i += stride)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "```\n",
        "\n",
        "**Why `stride = blockDim.x`?** We have 256 threads, each needs a unique starting point (`index`), and they must not overlap. By stepping by the total thread count, each element is processed exactly once:\n",
        "\n",
        "```\n",
        "Array:    [0] [1] [2] ... [255] [256] [257] ... [511] [512] ...\n",
        "Thread 0:  ^                      ^                     ^\n",
        "Thread 1:      ^                        ^                     ^\n",
        "Thread 2:          ^                          ^\n",
        "```\n",
        "\n",
        "If stride were smaller (say 128), threads 0 and 128 would both try to process element 256 - causing duplicate work or race conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add_block.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile add_block.cu\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "__global__\n",
        "void add(int n, float *x, float *y) {\n",
        "    int index = threadIdx.x;      // This thread's starting index\n",
        "    int stride = blockDim.x;      // Total threads = step size\n",
        "    for (int i = index; i < n; i += stride)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1<<30;  // 1 billion elements\n",
        "    float *x, *y;\n",
        "    \n",
        "    cudaMallocManaged(&x, N * sizeof(float));\n",
        "    cudaMallocManaged(&y, N * sizeof(float));\n",
        "    \n",
        "    for (int i = 0; i < N; i++) {\n",
        "        x[i] = 1.0f;\n",
        "        y[i] = 2.0f;\n",
        "    }\n",
        "    \n",
        "    // 1 block, 256 threads (better, but still limited)\n",
        "    add<<<1, 256>>>(N, x, y);\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < N; i++)\n",
        "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "    \n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)             Name           \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  --------------------------\n",
            "    100.0       3466668059          1  3466668059.0  3466668059.0  3466668059  3466668059          0.0  add(int, float *, float *)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)               Operation              \n",
            " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------------------------\n",
            "     68.3       1450967669  49152   29520.0    7151.5      1855    169629      48171.5  [CUDA memcpy Unified Host-to-Device]\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "/usr/local/cuda/bin/nvcc add_block.cu -o add_block\n",
        "nsys profile --stats=true ./add_block 2>&1 | grep -A 10 'cuda_gpu_kern_sum'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Total Time (ns) in above example is roughly 3 seconds. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Big speedup!** We went from 1 thread to 256. But GPUs have thousands of cores - let's use more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 8: Using Multiple Blocks\n",
        "\n",
        "GPUs have many **Streaming Multiprocessors (SMs)**, each running multiple thread blocks. For example, a Tesla P100 has 56 SMs, each supporting up to 2048 threads.\n",
        "\n",
        "To use all this power, we launch **multiple blocks**:\n",
        "\n",
        "```c\n",
        "int blockSize = 256;\n",
        "int numBlocks = (N + blockSize - 1) / blockSize;  // Round up\n",
        "add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "```\n",
        "\n",
        "The kernel uses a [grid-stride loop](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/) to handle arrays larger than the total thread count:\n",
        "\n",
        "```c\n",
        "__global__\n",
        "void add(int n, float *x, float *y) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;  // Total threads in grid\n",
        "    for (int i = index; i < n; i += stride)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting add_grid.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile add_grid.cu\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "__global__\n",
        "void init(int n, float *x, float *y) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    for (; i < n; i += stride) {\n",
        "        x[i] = 1.0f;\n",
        "        y[i] = 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void add(int n, float *x, float *y) {\n",
        "    // Global thread index\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    // Total threads in entire grid\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    \n",
        "    for (int i = index; i < n; i += stride)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1<<30;  // 1 billion elements\n",
        "    float *x, *y;\n",
        "    \n",
        "    cudaMallocManaged(&x, N * sizeof(float));\n",
        "    cudaMallocManaged(&y, N * sizeof(float));\n",
        "    \n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "    \n",
        "    init<<<numBlocks, blockSize>>>(N, x, y);\n",
        "    \n",
        "    printf(\"Launching %d blocks x %d threads\\n\", numBlocks, blockSize);\n",
        "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "    cudaDeviceSynchronize();\n",
        "    \n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < N; i++)\n",
        "        maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "    \n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching 4194304 blocks x 256 threads\n",
            "Max error: 0.000000\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "/usr/local/cuda/bin/nvcc add_grid.cu -o add_grid\n",
        "# nsys profile --stats=true ./add_grid 2>&1 | grep -A 10 'cuda_gpu_kern_sum'\n",
        "./add_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Another speedup!** We're now using the full power of the GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary: CPU to CUDA Cheat Sheet\n",
        "\n",
        "| What | CPU (C) | GPU (CUDA) |\n",
        "|------|---------|------------|\n",
        "| Function | `void func()` | `__global__ void func()` |\n",
        "| Allocate | `malloc(size)` | `cudaMallocManaged(&ptr, size)` |\n",
        "| Free | `free(ptr)` | `cudaFree(ptr)` |\n",
        "| Call | `func(args)` | `func<<<blocks, threads>>>(args)` |\n",
        "| Wait | (automatic) | `cudaDeviceSynchronize()` |\n",
        "| Thread ID | N/A | `blockIdx.x * blockDim.x + threadIdx.x` |\n",
        "| File | `.c` | `.cu` |\n",
        "| Compiler | `gcc` | `nvcc` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Resources\n",
        "\n",
        "**Documentation:**\n",
        "- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
        "- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)\n",
        "- [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/index.html)\n",
        "\n",
        "**Courses:**\n",
        "- [Fundamentals of Accelerated Computing with CUDA C/C++](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) - NVIDIA DLI\n",
        "- [Fundamentals of Accelerated Computing with CUDA Python](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about) - NVIDIA DLI\n",
        "\n",
        "**Tools:**\n",
        "- `nsys` - Nsight Systems command-line profiler (used in this notebook)\n",
        "- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) - Visual profiler\n",
        "- [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) - Kernel profiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "*Based on Mark Harris's [An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
